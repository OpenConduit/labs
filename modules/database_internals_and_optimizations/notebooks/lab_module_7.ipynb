{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd7a456",
   "metadata": {},
   "source": [
    "# Module 7: Join Algorithms\n",
    "## Goal: Understanding how the database \"zips\" data together.\n",
    "\n",
    "In module 6, we saw how the Optimizer estimates costs. Now, we see those estimates in action.\n",
    "\n",
    "The most expensive operation in any database is usually the **JOIN**. Combining two separate tables requires comparing rows to find matches. How the database physically does this depends heavily on the \"Physics\" of the data (Sorted? Small? Large?).\n",
    "\n",
    "There are three main algorithms you must master:\n",
    "1.  **Nested Loop Join:** The \"For Loop\" approach. Simple, but dangerous for large data.\n",
    "2.  **Hash Join:** The \"HashMap\" approach. The gold standard for unsorted data engineering.\n",
    "3.  **Merge Join:** The \"Zipper\" approach. Extremely fast, *if* the data is already sorted.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Connection\n",
    "\n",
    "We will load two tables:\n",
    "1.  `users_join`: 100,000 rows (The \"Dimension\" table).\n",
    "2.  `orders_join`: 500,000 rows (The \"Fact\" table).\n",
    "\n",
    "**Note:** We are creating a foreign key relationship where `orders_join.user_id` matches `users_join.user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb525ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "# Database Connection\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"db_int_opt\",\n",
    "    \"port\": 5432,\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"password\",\n",
    "    \"dbname\": \"db_int_opt\"\n",
    "}\n",
    "\n",
    "def get_db_connection():\n",
    "    conn = psycopg2.connect(**DB_PARAMS)\n",
    "    conn.autocommit = True\n",
    "    return conn\n",
    "\n",
    "conn = get_db_connection()\n",
    "cur = conn.cursor()\n",
    "print(\"✅ Connected to Postgres.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1cd956",
   "metadata": {},
   "source": [
    "#### 1.1 Load the Data\n",
    "We need to ensure the `orders` table has a `user_id` to join on. The `orders_sorted.csv` file typically contains `order_id`, `user_id`, `order_date`, amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3934ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# 1. Create Tables\n",
    "cur.execute(\"DROP TABLE IF EXISTS users_join CASCADE;\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS orders_join CASCADE;\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE users_join (\n",
    "        user_id INTEGER PRIMARY KEY, \n",
    "        name TEXT, \n",
    "        email TEXT, \n",
    "        account_status TEXT, \n",
    "        signup_date TIMESTAMP, \n",
    "        is_email_verified BOOLEAN\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE orders_join (\n",
    "        order_id INTEGER, \n",
    "        user_id INTEGER, \n",
    "        order_date DATE, \n",
    "        amount FLOAT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# 2. Load Users\n",
    "print(\"⏳ Loading users...\")\n",
    "df_users = pd.read_csv('../data/users.csv')\n",
    "buffer = StringIO()\n",
    "df_users.to_csv(buffer, index=False, header=False)\n",
    "buffer.seek(0)\n",
    "cur.copy_expert(\"COPY users_join FROM STDIN WITH CSV\", buffer)\n",
    "\n",
    "# 3. Load Orders (With Auto-Repair)\n",
    "print(\"⏳ Loading orders...\")\n",
    "df_orders = pd.read_csv('../data/orders_sorted.csv')\n",
    "print(f\"   Original Columns: {list(df_orders.columns)}\")\n",
    "\n",
    "# PATCH: Check for missing columns and generate data if needed\n",
    "if 'user_id' not in df_orders.columns:\n",
    "    print(\"   ⚠️ Injecting 'user_id' (Random 1-100k)...\")\n",
    "    df_orders['user_id'] = np.random.randint(1, 100000, df_orders.shape[0])\n",
    "\n",
    "if 'amount' not in df_orders.columns:\n",
    "    print(\"   ⚠️ Injecting 'amount' (Random $10-$500)...\")\n",
    "    df_orders['amount'] = np.random.uniform(10.0, 500.0, df_orders.shape[0]).round(2)\n",
    "\n",
    "# Ensure order matches database schema for COPY\n",
    "df_orders = df_orders[['order_id', 'user_id', 'order_date', 'amount']]\n",
    "\n",
    "buffer = StringIO()\n",
    "df_orders.to_csv(buffer, index=False, header=False)\n",
    "buffer.seek(0)\n",
    "cur.copy_expert(\"COPY orders_join FROM STDIN WITH CSV\", buffer)\n",
    "\n",
    "# 4. Analyze to update stats\n",
    "cur.execute(\"ANALYZE users_join;\")\n",
    "cur.execute(\"ANALYZE orders_join;\")\n",
    "print(\"✅ Data Loaded: 100k Users, 500k Orders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb93c7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1341d04",
   "metadata": {},
   "source": [
    "## 2. Experiment 7.1: The Nested Loop Join (The \"For Loop\")\n",
    "\n",
    "The Nested Loop is the simplest algorithm. It works exactly like writing two for loops in Python:\n",
    "\n",
    "```python\n",
    "for user in users:          # Outer Loop\n",
    "    for order in orders:    # Inner Loop\n",
    "        if user.id == order.user_id:\n",
    "            yield (user, order)\n",
    "```\n",
    "\n",
    "**The Physics**: If you have 100k users and 500k orders, that is $100,000 \\times 500,000 = 50,000,000,000$ comparisons. This is disastrous unless there is an index on the inner loop.\n",
    "\n",
    "We will force a Nested Loop Join. To be fair, we will add an index on `orders(user_id)` so we don't wait until the heat death of the universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807cec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Index to make Nested Loop viable\n",
    "cur.execute(\"CREATE INDEX IF NOT EXISTS idx_orders_user ON orders_join(user_id);\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT count(*) \n",
    "    FROM users_join u \n",
    "    JOIN orders_join o ON u.user_id = o.user_id\n",
    "\"\"\"\n",
    "\n",
    "# Force Nested Loop\n",
    "cur.execute(\"SET enable_nestloop = ON;\")\n",
    "cur.execute(\"SET enable_hashjoin = OFF;\")\n",
    "cur.execute(\"SET enable_mergejoin = OFF;\")\n",
    "\n",
    "print(\"⏳ Running Nested Loop Join (This might take a few seconds)...\")\n",
    "start_nl = time.time()\n",
    "cur.execute(f\"EXPLAIN (ANALYZE, FORMAT JSON) {query}\")\n",
    "result_nl = cur.fetchone()[0][0]\n",
    "end_nl = time.time()\n",
    "\n",
    "time_nl = (end_nl - start_nl) * 1000\n",
    "cost_nl = result_nl['Plan']['Total Cost']\n",
    "print(f\"✅ Nested Loop Done: {time_nl:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5cfbc",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e306599",
   "metadata": {},
   "source": [
    "## 3. Experiment 7.2: The Hash Join (The \"Engineering Standard\")\n",
    "The Hash Join is the default for most Data Engineering workloads.\n",
    "1. **Build Phase**: It takes the smaller table (Users) and builds a Hash Map in memory (RAM).\n",
    "    - `Key: user_id -> Value: Row Location`\n",
    "2. **Probe Phase**: It scans the larger table (Orders) once. For every row, it looks up the `user_id` in the Hash Map.\n",
    "\n",
    "**The Physics**: Complexity is $O(N + M)$. We touch every row exactly once. No looping back and forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94064f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Hash Join\n",
    "cur.execute(\"SET enable_nestloop = OFF;\")\n",
    "cur.execute(\"SET enable_hashjoin = ON;\")\n",
    "cur.execute(\"SET enable_mergejoin = OFF;\")\n",
    "\n",
    "print(\"⏳ Running Hash Join...\")\n",
    "start_hj = time.time()\n",
    "cur.execute(f\"EXPLAIN (ANALYZE, FORMAT JSON) {query}\")\n",
    "result_hj = cur.fetchone()[0][0]\n",
    "end_hj = time.time()\n",
    "\n",
    "time_hj = (end_hj - start_hj) * 1000\n",
    "cost_hj = result_hj['Plan']['Total Cost']\n",
    "print(f\"✅ Hash Join Done: {time_hj:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0be113",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fbaa23",
   "metadata": {},
   "source": [
    "## 4. Experiment 7.3: The Sort-Merge Join (The \"Zipper\")\n",
    "The Merge Join is beautiful but needy. It requires both tables to be physically sorted by the join key (`user_id`).\n",
    "1. It puts a pointer at the top of Table A and Table B.\n",
    "2. It moves them down together, \"zipping\" matches.\n",
    "\n",
    "**The Physics**: If the data is not sorted, the database must sort it first ($O(N \\log N)$), which is expensive.We will try this join without pre-sorting the data to see the cost of that \"Sort\" step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Merge Join\n",
    "cur.execute(\"SET enable_nestloop = OFF;\")\n",
    "cur.execute(\"SET enable_hashjoin = OFF;\")\n",
    "cur.execute(\"SET enable_mergejoin = ON;\")\n",
    "\n",
    "print(\"⏳ Running Merge Join...\")\n",
    "start_mj = time.time()\n",
    "cur.execute(f\"EXPLAIN (ANALYZE, FORMAT JSON) {query}\")\n",
    "result_mj = cur.fetchone()[0][0]\n",
    "end_mj = time.time()\n",
    "\n",
    "time_mj = (end_mj - start_mj) * 1000\n",
    "cost_mj = result_mj['Plan']['Total Cost']\n",
    "print(f\"✅ Merge Join Done: {time_mj:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7349fc1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd6b63",
   "metadata": {},
   "source": [
    "## 5. Visualization and Analysis\n",
    "Let's compare the three gladiators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcab52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['Nested Loop', 'Hash Join', 'Merge Join']\n",
    "times = [time_nl, time_hj, time_mj]\n",
    "costs = [cost_nl, cost_hj, cost_mj]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Chart 1: Execution Time\n",
    "sns.barplot(x=methods, y=times, ax=ax1, palette=\"viridis\")\n",
    "ax1.set_title(\"Actual Execution Time (ms)\")\n",
    "ax1.set_ylabel(\"Milliseconds (Lower is Better)\")\n",
    "for i, v in enumerate(times):\n",
    "    ax1.text(i, v + 10, f\"{v:.0f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# Chart 2: Optimizer Cost\n",
    "sns.barplot(x=methods, y=costs, ax=ax2, palette=\"magma\")\n",
    "ax2.set_title(\"Optimizer Cost Estimate\")\n",
    "ax2.set_ylabel(\"Cost Units\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Restore defaults\n",
    "cur.execute(\"SET enable_nestloop = ON;\")\n",
    "cur.execute(\"SET enable_hashjoin = ON;\")\n",
    "cur.execute(\"SET enable_mergejoin = ON;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a4168",
   "metadata": {},
   "source": [
    "## 5.1 The \"Why\" behind the results\n",
    "1. **Nested Loop (The Villain):**\n",
    "    - **Expected**: It was the slowest (74ms) and had the highest predicted cost.\n",
    "    - **Why**: Even with the index, the CPU had to jump back and forth 500,000 times. The Optimizer correctly identified this as the worst option (look at that massive purple bar!).\n",
    "2. **Hash Join (The Optimizer's Favorite):**\n",
    "    - **Expected**: The Optimizer gave this the lowest cost (the shortest purple bar). This means if you hadn't forced anything, the database would have chosen this path.\n",
    "    - **Why**: Hash Joins are the \"safe bet\" for large, unsorted data.\n",
    "3. **Merge Join (The Surprise Winner)**:\n",
    "    - **Unexpected but Logical**: It was actually the fastest (47ms), beating the Hash Join (60ms), even though the Optimizer thought it would be more expensive.\n",
    "    - **The Physics**: The \"Sort\" step (required for Merge Join) turned out to be cheaper than the Optimizer feared. Modern CPUs are incredibly fast at sorting simple integers (like `user_id`) in L2/L3 Cache. The Optimizer was conservative, expecting the sort to be slower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a3f479",
   "metadata": {},
   "source": [
    "# Module 4: The B-Tree\n",
    "**Goal**: Shatter the illusion that Indexes are \"Magic Fast Buttons.\"\n",
    "Most developers know that adding an index makes queries fast, but they don't know why. It isn't magic; it is a specific data structure that creates a tradeoff: you trade write speed (maintaining order) for read speed (finding data quickly).\n",
    "\n",
    "In this module, we will explore:\n",
    "1. **The Algorithm**: Why sorting data transforms a search from \"checking every page\" to \"flipping a few pages.\"\n",
    "2. **The Physics**: Observing Postgres skipping thousands of disk pages using an Index Scan.\n",
    "3. **The Clustered Index**: How physical disk sorting impacts range queries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68e3bd",
   "metadata": {},
   "source": [
    "## 0. Setup: The Laboratory\n",
    "We will use **Postgres** for this module, as it provides excellent tooling (EXPLAIN ANALYZE) to see exactly how the database engine is reading data from the disk.\n",
    "\n",
    "We will prepare two tables using our users dataset:\n",
    "1. `users_heap`: A standard table with no index.\n",
    "2. `users_btree`: The same data, but with a B-Tree Index on `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Database Connection\n",
    "conn = psycopg2.connect(\n",
    "    host=\"db_int_opt\",\n",
    "    user=\"admin\",\n",
    "    password=\"password\",\n",
    "    dbname=\"db_int_opt\"\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Visualization Settings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "print(\"âœ… Laboratory Environment Ready.\")\n",
    "\n",
    "# --- DATA PREPARATION ---\n",
    "print(\"âš™ï¸ Loading Data into Postgres...\")\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('../data/users.csv')\n",
    "\n",
    "if 'status' in df.columns:\n",
    "    df.rename(columns={'status': 'account_status'}, inplace=True)\n",
    "\n",
    "df_clean = df[['user_id', 'account_status', 'email']]\n",
    "\n",
    "# Create tables\n",
    "cur.execute(\"DROP TABLE IF EXISTS users_heap;\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS users_btree;\")\n",
    "\n",
    "# Table 1: No Index (The Heap)\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE users_heap (\n",
    "        user_id INT,\n",
    "        account_status TEXT,\n",
    "        email TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Table 2: With Index (The B-Tree)\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE users_btree (\n",
    "        user_id INT,\n",
    "        account_status TEXT,\n",
    "        email TEXT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# Bulk Insert (Using COPY for speed)\n",
    "from io import StringIO\n",
    "buffer = StringIO()\n",
    "df_clean.to_csv(buffer, index=False, header=False)\n",
    "buffer.seek(0)\n",
    "\n",
    "# Load into both\n",
    "cur.copy_expert(\"COPY users_heap FROM STDIN WITH CSV\", buffer)\n",
    "buffer.seek(0)\n",
    "cur.copy_expert(\"COPY users_btree FROM STDIN WITH CSV\", buffer)\n",
    "\n",
    "# CREATE THE INDEX on the second table\n",
    "print(\"âš™ï¸ Building B-Tree Index...\")\n",
    "cur.execute(\"CREATE INDEX idx_user_id ON users_btree(user_id);\")\n",
    "# Force Postgres to gather statistics immediately so our experiments are accurate\n",
    "cur.execute(\"ANALYZE users_heap;\")\n",
    "cur.execute(\"ANALYZE users_btree;\")\n",
    "\n",
    "print(\"âœ… Data Loaded: 100k Rows in 'users_heap' and 'users_btree'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765cd327",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fdd1d",
   "metadata": {},
   "source": [
    "## 4.1 The Sorted Structure (The Phone Book)\n",
    "Before we look at the database, we must understand the math.\n",
    "\n",
    "Imagine I ask you to find the name \"Zoltan\" in a phone book.\n",
    "- **Scenario A (Random)**: The names are printed in random order. You must read every single name from the start until you find \"Zoltan\".\n",
    "- **Scenario B (Sorted)**: The names are alphabetical. You open the book to the middle. If you see \"M\", you know \"Z\" is in the right half. You split the remaining pages in half again.\n",
    "\n",
    "This acts as a \"filter.\" Every time you make a decision, you discard **half** of the remaining data.\n",
    "\n",
    "### Experiment 4.1: Linear Scan vs. Binary Search\n",
    "We will simulate this in Python memory. We will search for a number in a list of 100,000 integers.\n",
    "\n",
    "**Hypothesis**: How much faster is Binary Search? 2x? 10x?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33dc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect \n",
    "import random\n",
    "\n",
    "# 1. Prepare Data\n",
    "ids_list = df['user_id'].tolist()\n",
    "target_id = ids_list[-1] # Let's look for the very last item (Worst case for linear)\n",
    "\n",
    "# 2. Linear Search (The \"Scan\")\n",
    "start_linear = time.time()\n",
    "found = False\n",
    "for i in ids_list:\n",
    "    if i == target_id:\n",
    "        found = True\n",
    "        break\n",
    "end_linear = time.time()\n",
    "\n",
    "# 3. Binary Search (The \"Seek\")\n",
    "# We must sort the list first to use binary search\n",
    "ids_sorted = sorted(ids_list) \n",
    "\n",
    "start_binary = time.time()\n",
    "# bisect is Python's built-in binary search algorithm\n",
    "index = bisect.bisect_left(ids_sorted, target_id)\n",
    "end_binary = time.time()\n",
    "\n",
    "# 4. Results\n",
    "time_linear = (end_linear - start_linear) * 1000 # to ms\n",
    "time_binary = (end_binary - start_binary) * 1000 # to ms\n",
    "\n",
    "print(f\"Linear Search Time: {time_linear:.4f} ms\")\n",
    "print(f\"Binary Search Time: {time_binary:.4f} ms\")\n",
    "print(f\"Speedup Factor: {time_linear / time_binary:.0f}x\")\n",
    "\n",
    "# Visualize\n",
    "plt.bar(['Linear Scan (Unsorted)', 'Binary Search (Sorted)'], [time_linear, time_binary], color=['red', 'green'])\n",
    "plt.title(\"Search Algorithm Efficiency (Lower is Better)\")\n",
    "plt.ylabel(\"Time (milliseconds)\")\n",
    "plt.yscale('log') # Log scale because the difference is massive\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde68c82",
   "metadata": {},
   "source": [
    "## Observation\n",
    "You likely see a massive difference. Linear search performance degrades linearly ($O(n)$) as data grows. Binary search is Logarithmic ($O(\\log n)$). Even if we had 1 billion rows, the binary search would only take about 30 steps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77b506",
   "metadata": {},
   "source": [
    "## 4.2 Logarithmic Time ($O(\\log n)$) in the Database\n",
    "Now, let's see if this physics applies to the actual Postgres database engine.\n",
    "\n",
    "We will query for a specific `user_id`.\n",
    "- `users_heap`: Postgres must scan the whole table (Sequential Scan).\n",
    "- `users_btree`: Postgres will traverse the B-Tree index to find the pointer to the specific row.\n",
    "\n",
    "### Experiment 4.2: Sequential Scan vs. Index Scan\n",
    "\n",
    "We will use the SQL command `EXPLAIN (ANALYZE, BUFFERS)` to ask Postgres exactly how much work it did. We are looking for \"Buffers\"â€”this is the number of 8KB pages Postgres had to read from memory/disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312835d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_query_cost(table_name, search_id):\n",
    "    query = f\"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) SELECT * FROM {table_name} WHERE user_id = {search_id};\"\n",
    "    cur.execute(query)\n",
    "    plan = cur.fetchone()[0][0]\n",
    "    \n",
    "    cost = plan['Plan']['Total Cost']\n",
    "    execution_time = plan['Execution Time']\n",
    "    # Extract buffer hits (shared blocks hit)\n",
    "    buffers = plan['Plan'].get('Shared Hit Blocks', 0) + plan['Plan'].get('Shared Read Blocks', 0)\n",
    "    \n",
    "    return execution_time, buffers\n",
    "\n",
    "# Search for a random ID\n",
    "search_target = 99999\n",
    "\n",
    "time_heap, buffers_heap = measure_query_cost(\"users_heap\", search_target)\n",
    "time_btree, buffers_btree = measure_query_cost(\"users_btree\", search_target)\n",
    "\n",
    "print(f\"HEAP TABLE: {time_heap:.3f} ms | Pages Read: {buffers_heap}\")\n",
    "print(f\"BTREE TABLE: {time_btree:.3f} ms | Pages Read: {buffers_btree}\")\n",
    "\n",
    "# Visualize Pages Read (The Physical Work)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['Heap (Seq Scan)', 'B-Tree (Index Scan)'], [buffers_heap, buffers_btree], color=['#e74c3c', '#2ecc71'])\n",
    "plt.title(\"Physical I/O Cost: Pages Read from Memory/Disk\")\n",
    "plt.ylabel(\"Number of 8KB Pages\")\n",
    "for i, v in enumerate([buffers_heap, buffers_btree]):\n",
    "    plt.text(i, v + 1, str(v), ha='center', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874fd73",
   "metadata": {},
   "source": [
    "### The Physics: Why so few pages?\n",
    "The B-Tree is a short, wide tree.\n",
    "1. **Root Node**: Postgres reads the top page. It says \"IDs 0-50k go Left, 50k-100k go Right.\"\n",
    "2. **Branch Nodes**: It follows the pointer down.\n",
    "3. **Leaf Node**: It hits the bottom, finds the exact location of the data on the disk (Tuple ID).\n",
    "4. **The Table**: It jumps to the specific Heap page to grab the row.\n",
    "\n",
    "This usually takes 3 to 4 \"hops\" (Page reads), regardless of whether the table has 100k rows or 10M rows. The Heap Scan, conversely, had to read every single page hoping to find the ID.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae162379",
   "metadata": {},
   "source": [
    "## 4.3 The Clustered Index (Physical Sorting)\n",
    "An index helps you find a single needle. But what if you want a range of needles?\n",
    "\n",
    "If your data is physically scattered on the disk (Random), fetching a range means jumping around frantically (Random I/O). If your data is physically sorted (Clustered), fetching a range is a smooth sequential read.\n",
    "\n",
    "**Note**: Postgres does not automatically maintain Clustered Indexes (unlike SQL Server or MySQL InnoDB), but we can simulate it by loading data in sorted order vs. shuffled order.\n",
    "\n",
    "### Experiment 4.3: Range Query on Sorted vs. Shuffled Data\n",
    "We will load `orders_sorted.csv` and `orders_shuffled.csv`. Both contain the same data. We will query a date range.\n",
    "\n",
    "**Hypothesis**: The sorted table will be faster because the data for \"January 2023\" resides on adjacent pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DEFINE SCHEMA & TABLES ---\n",
    "cur.execute(\"DROP TABLE IF EXISTS orders_sorted;\")\n",
    "cur.execute(\"DROP TABLE IF EXISTS orders_shuffled;\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE orders_sorted (\n",
    "        order_id INT,\n",
    "        user_id INT,\n",
    "        order_date TIMESTAMP,\n",
    "        amount FLOAT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE orders_shuffled (\n",
    "        order_id INT,\n",
    "        user_id INT,\n",
    "        order_date TIMESTAMP,\n",
    "        amount FLOAT\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "# --- 3. ROBUST DATA LOADING (CLEANING) ---\n",
    "def clean_and_load(file_path, table_name):\n",
    "    print(f\"   Processing {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Handle missing 'amount' column by faking it or using quantity\n",
    "    if 'amount' not in df.columns:\n",
    "        if 'quantity' in df.columns:\n",
    "             df['amount'] = df['quantity'] * 10.0 \n",
    "        else:\n",
    "             df['amount'] = 0.0\n",
    "\n",
    "    # FORCE column selection and order to match SQL exactly\n",
    "    # This prevents the \"BadCopyFileFormat\" error\n",
    "    df_clean = df[['order_id', 'user_id', 'order_date', 'amount']]\n",
    "    \n",
    "    # Stream to DB\n",
    "    buffer = StringIO()\n",
    "    df_clean.to_csv(buffer, index=False, header=False)\n",
    "    buffer.seek(0)\n",
    "    cur.copy_expert(f\"COPY {table_name} FROM STDIN WITH CSV\", buffer)\n",
    "\n",
    "clean_and_load('../data/orders_sorted.csv', 'orders_sorted')\n",
    "clean_and_load('../data/orders_shuffled.csv', 'orders_shuffled')\n",
    "\n",
    "# --- 4. BUILD INDEXES ---\n",
    "print(\"âš™ï¸ Building Indexes (The 'Phone Book')...\")\n",
    "cur.execute(\"CREATE INDEX idx_date_sorted ON orders_sorted(order_date);\")\n",
    "cur.execute(\"CREATE INDEX idx_date_shuffled ON orders_shuffled(order_date);\")\n",
    "cur.execute(\"ANALYZE orders_sorted; ANALYZE orders_shuffled;\")\n",
    "\n",
    "# --- 5. DYNAMIC RANGE FINDING ---\n",
    "# Find a date that actually exists in the data\n",
    "cur.execute(\"SELECT order_date FROM orders_sorted ORDER BY order_date LIMIT 1 OFFSET 25000;\")\n",
    "start_date_raw = cur.fetchone()[0]\n",
    "\n",
    "# Ensure it's a timestamp object\n",
    "if isinstance(start_date_raw, str):\n",
    "    start_date_obj = pd.to_datetime(start_date_raw)\n",
    "else:\n",
    "    start_date_obj = start_date_raw\n",
    "\n",
    "# Define a NARROW range (1 Day) to encourage Index usage\n",
    "end_date_obj = start_date_obj + pd.DateOffset(days=1)\n",
    "\n",
    "start_str = start_date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "end_str = end_date_obj.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(f\"ðŸŽ¯ Targeted Range: {start_str} to {end_str}\")\n",
    "\n",
    "# --- 6. EXECUTE EXPERIMENT (FORCED INDEX) ---\n",
    "# We disable Sequential Scan to force the database to use the index,\n",
    "# revealing the true cost of \"hopping\" around the disk.\n",
    "cur.execute(\"SET enable_seqscan = OFF;\")\n",
    "\n",
    "query_template = f\"\"\"\n",
    "    EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) \n",
    "    SELECT * FROM {{}} \n",
    "    WHERE order_date BETWEEN '{start_str}' AND '{end_str}';\n",
    "\"\"\"\n",
    "\n",
    "def measure_cost(table):\n",
    "    cur.execute(query_template.format(table))\n",
    "    plan = cur.fetchone()[0][0]\n",
    "    buffers = plan['Plan']['Shared Hit Blocks'] + plan['Plan']['Shared Read Blocks']\n",
    "    return plan['Execution Time'], buffers\n",
    "\n",
    "print(\"\\nðŸš€ Running Physics Experiment...\")\n",
    "t_sorted, b_sorted = measure_cost(\"orders_sorted\")\n",
    "t_shuffled, b_shuffled = measure_cost(\"orders_shuffled\")\n",
    "\n",
    "# Reset config\n",
    "cur.execute(\"SET enable_seqscan = ON;\")\n",
    "\n",
    "print(f\"Sorted Table:   {t_sorted:.3f} ms | Pages Read: {b_sorted}\")\n",
    "print(f\"Shuffled Table: {t_shuffled:.3f} ms | Pages Read: {b_shuffled}\")\n",
    "\n",
    "# --- 7. VISUALIZATION ---\n",
    "fig, ax = plt.subplots(figsize=(8,5)) \n",
    "\n",
    "# Plot\n",
    "bars = ax.bar(['Physically Sorted', 'Physically Shuffled'], [t_sorted, t_shuffled], color=['#3498db', '#e67e22'])\n",
    "\n",
    "# Styling\n",
    "ax.set_ylabel('Execution Time (ms)')\n",
    "ax.set_title(f'The \"Clustering Factor\": Impact of Physical Order', pad=20)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add Data Labels\n",
    "for bar, pages in zip(bars, [b_sorted, b_shuffled]):\n",
    "    height = bar.get_height()\n",
    "    label_height = height if height > 0 else 0.1 # prevention for 0 height\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., label_height,\n",
    "            f'{height:.1f} ms\\n({pages} Pages)',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde96c9",
   "metadata": {},
   "source": [
    "### Conclusion: Correlation\n",
    "In the **Sorted** table, the index points to rows that are sitting next to each other on the disk. The database reads one block and gets 50 relevant rows. In the **Shuffled** table, the index points to rows scattered everywhere. The database reads a block, finds 1 row, and has to jump to a new block for the next row.\n",
    "\n",
    "Take a second to really appreciate the difference in pages between the two searches. We know now that a Postgres page is 8 KB, and the shuffled search required loading over 3,000 more pages. That equates to over a 24 MB increase in I/O for the shuffled data.\n",
    "\n",
    "This is why **Clustered Indexes** (or partitioning by time) are critical for time-series data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

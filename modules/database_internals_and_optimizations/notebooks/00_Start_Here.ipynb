{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdbb191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning existing directory: ../data...\n",
      "âœ¨ Directory cleaned: ../data\n",
      "ðŸ‘¤ Generating 100000 Users...\n",
      "   -> Saved users.csv and users.parquet\n",
      "ðŸ“¦ Generating 1000 Products...\n",
      "ðŸ›’ Generating 500000 Orders (Uniform Distribution)...\n",
      "   -> Saved orders_sorted.csv (Sequential I/O) and orders_shuffled.csv (Random I/O)\n",
      "ðŸ–±ï¸ Generating 2000000 Click Events (Highly Skewed)...\n",
      "   -> Saved clickstream.parquet (Contains the 'Justin Bieber' skew)\n",
      "\n",
      "âœ… DATA GENERATION COMPLETE. The Universe is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"../data\"\n",
    "SEED = 42  # The God Constant: Ensures every student gets the exact same \"random\" data\n",
    "NUM_USERS = 100_000\n",
    "NUM_PRODUCTS = 1_000\n",
    "NUM_ORDERS = 500_000\n",
    "NUM_CLICKS = 2_000_000\n",
    "\n",
    "# Initialize\n",
    "fake = Faker()\n",
    "Faker.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "def setup_directories():\n",
    "    # If the directory doesn't exist, create it\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "        print(f\"ðŸ“‚ Created new directory: {DATA_DIR}\")\n",
    "        return\n",
    "\n",
    "    # If it exists (and might be a mount point), empty it carefully\n",
    "    print(f\"ðŸ§¹ Cleaning existing directory: {DATA_DIR}...\")\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path) # Delete file\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path) # Delete subfolder\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to delete {file_path}. Reason: {e}\")\n",
    "            \n",
    "    print(f\"âœ¨ Directory cleaned: {DATA_DIR}\")\n",
    "\n",
    "# --- 1. USERS (The Dimension Table) ---\n",
    "def generate_users():\n",
    "    print(f\"ðŸ‘¤ Generating {NUM_USERS} Users...\")\n",
    "    \n",
    "    # We need a mix of high and low cardinality for Indexing/Compression demos\n",
    "    data = {\n",
    "        'user_id': np.arange(1, NUM_USERS + 1),\n",
    "        'name': [fake.name() for _ in range(NUM_USERS)],\n",
    "        'email': [fake.unique.email() for _ in range(NUM_USERS)],\n",
    "        \n",
    "        # LOW CARDINALITY: Perfect for Bitmap Index & RLE Compression demos\n",
    "        # Only 3 unique values across 100k rows\n",
    "        'account_status': np.random.choice(['ACTIVE', 'INACTIVE', 'PREMIUM'], NUM_USERS, p=[0.70, 0.25, 0.05]),\n",
    "        \n",
    "        # HIGH CARDINALITY: Good for B-Tree demos\n",
    "        'signup_date': [fake.date_time_this_decade() for _ in range(NUM_USERS)],\n",
    "        \n",
    "        # BOOLEAN: For \"Bit packing\" demos\n",
    "        'is_email_verified': np.random.choice([True, False], NUM_USERS)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save as CSV (Row-based, slow to read)\n",
    "    df.to_csv(f\"{DATA_DIR}/users.csv\", index=False)\n",
    "    \n",
    "    # Save as Parquet (Column-based, fast to read)\n",
    "    df.to_parquet(f\"{DATA_DIR}/users.parquet\")\n",
    "    print(\"   -> Saved users.csv and users.parquet\")\n",
    "    return df\n",
    "\n",
    "# --- 2. PRODUCTS (The Small Table) ---\n",
    "def generate_products():\n",
    "    print(f\"ðŸ“¦ Generating {NUM_PRODUCTS} Products...\")\n",
    "    \n",
    "    data = {\n",
    "        'product_id': np.arange(1, NUM_PRODUCTS + 1),\n",
    "        'product_name': [fake.catch_phrase() for _ in range(NUM_PRODUCTS)],\n",
    "        'category': np.random.choice(['Electronics', 'Books', 'Home', 'Garden', 'Toys'], NUM_PRODUCTS),\n",
    "        'price': np.round(np.random.uniform(5.0, 500.0, NUM_PRODUCTS), 2)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f\"{DATA_DIR}/products.csv\", index=False)\n",
    "    df.to_parquet(f\"{DATA_DIR}/products.parquet\")\n",
    "    return df\n",
    "\n",
    "# --- 3. ORDERS (The Fact Table - Uniform Distribution) ---\n",
    "def generate_orders(users_df, products_df):\n",
    "    print(f\"ðŸ›’ Generating {NUM_ORDERS} Orders (Uniform Distribution)...\")\n",
    "    \n",
    "    # Randomly assign users and products\n",
    "    user_ids = users_df['user_id'].values\n",
    "    product_ids = products_df['product_id'].values\n",
    "    \n",
    "    data = {\n",
    "        'order_id': np.arange(1, NUM_ORDERS + 1),\n",
    "        'user_id': np.random.choice(user_ids, NUM_ORDERS),\n",
    "        'product_id': np.random.choice(product_ids, NUM_ORDERS),\n",
    "        'quantity': np.random.randint(1, 5, NUM_ORDERS),\n",
    "        'order_date': [fake.date_time_this_year() for _ in range(NUM_ORDERS)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # SORTED VERSION: For \"Clustered Index\" & \"Sequential Read\" demos\n",
    "    # Reading this from disk will be fast because it's physically ordered by date\n",
    "    df_sorted = df.sort_values(by='order_date')\n",
    "    df_sorted.to_csv(f\"{DATA_DIR}/orders_sorted.csv\", index=False)\n",
    "    \n",
    "    # SHUFFLED VERSION: For \"Heap Scan\" & \"Random Seek\" demos\n",
    "    # Reading this chronologically will force the disk head to jump around\n",
    "    df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "    df_shuffled.to_csv(f\"{DATA_DIR}/orders_shuffled.csv\", index=False)\n",
    "    \n",
    "    print(\"   -> Saved orders_sorted.csv (Sequential I/O) and orders_shuffled.csv (Random I/O)\")\n",
    "\n",
    "# --- 4. CLICKSTREAM (The Big Data - SKEWED Distribution) ---\n",
    "def generate_clickstream(users_df):\n",
    "    print(f\"ðŸ–±ï¸ Generating {NUM_CLICKS} Click Events (Highly Skewed)...\")\n",
    "    \n",
    "    # THE SKEW: We want to break distributed joins (Shuffle).\n",
    "    # User ID 1 (The \"Bot\") generates 50% of all traffic.\n",
    "    # The other 50% is spread among the remaining 99,999 users.\n",
    "    \n",
    "    skew_count = int(NUM_CLICKS * 0.50)\n",
    "    normal_count = NUM_CLICKS - skew_count\n",
    "    \n",
    "    heavy_hitter = np.full(skew_count, 1) # 1 Million clicks for User #1\n",
    "    long_tail = np.random.choice(users_df['user_id'].values, normal_count)\n",
    "    \n",
    "    # Combine and shuffle so the file looks \"random\" but is statistically cursed\n",
    "    all_users = np.concatenate([heavy_hitter, long_tail])\n",
    "    np.random.shuffle(all_users)\n",
    "    \n",
    "    data = {\n",
    "        'event_id': np.arange(1, NUM_CLICKS + 1),\n",
    "        'user_id': all_users,\n",
    "        'url': [fake.uri_path() for _ in range(NUM_CLICKS)],\n",
    "        'timestamp': [fake.date_time_this_year() for _ in range(NUM_CLICKS)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Partitioned Write: Useful for \"Partition Pruning\" demos\n",
    "    # We save this as parquet\n",
    "    df.to_parquet(f\"{DATA_DIR}/clickstream.parquet\")\n",
    "    print(\"   -> Saved clickstream.parquet (Contains the 'Justin Bieber' skew)\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "if __name__ == \"__main__\":\n",
    "    setup_directories()\n",
    "    users = generate_users()\n",
    "    products = generate_products()\n",
    "    generate_orders(users, products)\n",
    "    generate_clickstream(users)\n",
    "    print(\"\\nâœ… DATA GENERATION COMPLETE. The Universe is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4458b-a839-4cc7-bf79-cad48d387eef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

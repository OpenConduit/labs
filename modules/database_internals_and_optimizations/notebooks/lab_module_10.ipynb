{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54e4d0b",
   "metadata": {},
   "source": [
    "# Module 10: Database Hygiene\n",
    "**Goal**: Understanding that deleting data doesn't actually remove it.\n",
    "In the physical world, if you erase a sentence from a whiteboard, the ink is gone. In the database world, \"deleting\" usually just means turning off the visibility switch. The data remains on the disk, taking up space, until a garbage collector comes along to clean it up.\n",
    "\n",
    "This chapter explores Entropy: the tendency of databases to get slower and larger over time unless actively maintained\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98c500",
   "metadata": {},
   "source": [
    "## 1. Setup and Tools\n",
    "We will use Postgres to demonstrate \"Dead Tuples\" (the cost of MVCC) and DuckDB/Parquet to demonstrate the \"Small Files Problem\" (a common issue in Data Lakes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06153dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Postgres Connection\n",
    "DB_PARAMS = {\n",
    "    \"host\": \"db_int_opt\",\n",
    "    \"port\": 5432,\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"password\",\n",
    "    \"dbname\": \"db_int_opt\"\n",
    "}\n",
    "\n",
    "# Cleanup function for Postgres\n",
    "def reset_postgres_table():\n",
    "    with psycopg2.connect(**DB_PARAMS) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"DROP TABLE IF EXISTS sensor_logs;\")\n",
    "            # Disable autovacuum so we can see the mess accumulate manually\n",
    "            cur.execute(\"\"\"\n",
    "                CREATE TABLE sensor_logs (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    payload TEXT\n",
    "                ) WITH (autovacuum_enabled = false);\n",
    "            \"\"\")\n",
    "        conn.commit()\n",
    "\n",
    "# Helper to get table size in MB\n",
    "def get_table_size(conn):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"SELECT pg_relation_size('sensor_logs') / 1024 / 1024.0;\")\n",
    "        return cur.fetchone()[0]\n",
    "\n",
    "print(\"Setup Complete. Tools Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a212373",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad9cde",
   "metadata": {},
   "source": [
    "## 2. Experiment 10.1: The Zombie Data (Dead Tuples)\n",
    "**The Concept**: Postgres uses MVCC. When you `DELETE` a row, Postgres doesn't overwrite the data with zeros. It simply marks the row header: \"Valid until Transaction X.\" Future transactions see that the row is \"expired\" and ignore it. However, the row still occupies bytes on the hard drive. These are called Dead Tuples.\n",
    "\n",
    "#### Step 1: Hypothesis\n",
    "If we insert 200,000 rows, measure the size, and then `DELETE` 100,000 rows, what happens to the table size on disk?\n",
    "- A) It drops by 50%.\n",
    "- B) It stays exactly the same.\n",
    "\n",
    "#### Step 2: The Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run the Experiment ---\n",
    "reset_postgres_table()\n",
    "\n",
    "sizes = []\n",
    "phases = []\n",
    "\n",
    "# Connect manually with autocommit=True for VACUUM compatibility\n",
    "conn = psycopg2.connect(**DB_PARAMS)\n",
    "conn.autocommit = True \n",
    "\n",
    "try:\n",
    "    # 1. Insert 200k Rows\n",
    "    print(\"[Step 1] Inserting 200k rows...\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"INSERT INTO sensor_logs (payload) SELECT md5(random()::text) FROM generate_series(1, 200000);\")\n",
    "    \n",
    "    size_1 = get_table_size(conn)\n",
    "    sizes.append(size_1)\n",
    "    phases.append(\"1. Initial\")\n",
    "    print(f\"Table Size: {size_1:.2f} MB\")\n",
    "\n",
    "    # 2. Delete 100k Rows (The \"Zombie\" Phase)\n",
    "    print(\"[Step 2] Deleting 100k rows...\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"DELETE FROM sensor_logs WHERE id <= 100000;\")\n",
    "    \n",
    "    # Wait for disk flush\n",
    "    time.sleep(1)\n",
    "    \n",
    "    size_2 = get_table_size(conn)\n",
    "    sizes.append(size_2)\n",
    "    phases.append(\"2. After Delete\")\n",
    "    print(f\"Table Size: {size_2:.2f} MB\")\n",
    "\n",
    "    # 3. VACUUM FULL\n",
    "    print(\"[Step 3] Running VACUUM FULL...\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"VACUUM FULL sensor_logs;\")\n",
    "        \n",
    "    size_3 = get_table_size(conn)\n",
    "    sizes.append(size_3)\n",
    "    phases.append(\"3. After Vacuum\")\n",
    "    print(f\"Table Size: {size_3:.2f} MB\")\n",
    "\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb062b7d",
   "metadata": {},
   "source": [
    "#### Step 3: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e974323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "# Use hue=phases to avoid future seaborn warnings, set legend=False\n",
    "sns.barplot(x=phases, y=sizes, hue=phases, palette=['blue', 'red', 'green'], legend=False)\n",
    "plt.title('The Myth of Deletion (Disk Usage)')\n",
    "plt.ylabel('Size on Disk (MB)')\n",
    "\n",
    "# Add labels (Now safe because v is a float)\n",
    "for i, v in enumerate(sizes):\n",
    "    plt.text(i, v + Decimal(0.1), f\"{v:.2f} MB\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ee008",
   "metadata": {},
   "source": [
    "#### Step 4: The Physics\n",
    "**Why didn't the size shrink?** In Phase 2, the table contained 50% \"Live\" data and 50% \"Dead\" data. The disk blocks were still full of bytes, but half of them were marked \"invisible.\"\n",
    "- **Standard** `VACUUM`: Scans the file, marks the dead space as \"reusable\" for future inserts, but usually does not return space to the OS.\n",
    "- `VACUUM FULL`: actually creates a brand new copy of the table file, packs the live rows tightly, and deletes the old file. This is why the size dropped in Phase 3.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e25328",
   "metadata": {},
   "source": [
    "## 3. Experiment 10.2: The Small Files Problem (Compaction)\n",
    "**The Concept**: In Data Lakes (S3, Parquet, Delta Lake), a common mistake is streaming data in tiny batches. This creates thousands of small files (KB size). Reading 1,000 files of 1KB is orders of magnitude slower than reading 1 file of 1MB, due to the overhead of opening files and parsing metadata headers.\n",
    "\n",
    "#### Step 1: Hypothesis\n",
    "We will write the exact same data (100k rows) in two ways:\n",
    "1. **Fragmented**: 100 files (1,000 rows each).\n",
    "2. **Compacted**: 1 file (100,000 rows). Which one will DuckDB query faster?\n",
    "\n",
    "#### Step 2: The Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678062b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "DATA_DIR = \"./data_hygiene\"\n",
    "if os.path.exists(DATA_DIR):\n",
    "    shutil.rmtree(DATA_DIR)\n",
    "os.makedirs(f\"{DATA_DIR}/fragmented\")\n",
    "os.makedirs(f\"{DATA_DIR}/compacted\")\n",
    "\n",
    "# Create a dummy DataFrame\n",
    "df = pd.DataFrame({'id': range(100000), 'data': 'x' * 100})\n",
    "\n",
    "print(\"Generating Fragmented Files (Please wait)...\")\n",
    "# Write 100 small files\n",
    "for i in range(100):\n",
    "    subset = df.iloc[i*1000 : (i+1)*1000]\n",
    "    subset.to_parquet(f\"{DATA_DIR}/fragmented/part_{i}.parquet\")\n",
    "\n",
    "print(\"Generating Compacted File...\")\n",
    "# Write 1 big file\n",
    "df.to_parquet(f\"{DATA_DIR}/compacted/full.parquet\")\n",
    "\n",
    "# Measurement\n",
    "con = duckdb.connect()\n",
    "times = []\n",
    "\n",
    "# 1. Read Fragmented\n",
    "start = time.time()\n",
    "con.execute(f\"SELECT COUNT(*) FROM read_parquet('{DATA_DIR}/fragmented/*.parquet')\").fetchall()\n",
    "t_frag = time.time() - start\n",
    "times.append(t_frag)\n",
    "print(f\"Read Fragmented: {t_frag:.4f}s\")\n",
    "\n",
    "# 2. Read Compacted\n",
    "start = time.time()\n",
    "con.execute(f\"SELECT COUNT(*) FROM read_parquet('{DATA_DIR}/compacted/full.parquet')\").fetchall()\n",
    "t_compact = time.time() - start\n",
    "times.append(t_compact)\n",
    "print(f\"Read Compacted:  {t_compact:.4f}s\")\n",
    "\n",
    "shutil.rmtree(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd0f82",
   "metadata": {},
   "source": [
    "#### Step 3: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccbd4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=['100 Small Files', '1 Big File'], y=times, palette=['orange', 'green'])\n",
    "plt.title('The Cost of Fragmentation (Read Latency)')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd7770",
   "metadata": {},
   "source": [
    "#### Step 4: The Physics\n",
    "Every file has a \"Header\" and a \"Footer\" (metadata).\n",
    "- **Fragmented**: The engine had to perform 100 open() syscalls, parse 100 headers, and issue 100 separate read requests.\n",
    "- **Compacted**: The engine opened 1 file, read 1 header, and streamed the data sequentially. Compaction is the process of merging these small files into larger ones (often 128MB - 1GB) to optimize read performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15617345",
   "metadata": {},
   "source": [
    "## 4. Experiment 10.3: Statistics Drift (Flying Blind)\n",
    "**The Concept**: The Query Optimizer (Chapter 6) doesn't count rows before every query (that would be slow). Instead, it looks at a \"Cheat Sheet\" called Statistics. If you add/delete data but don't update the cheat sheet (via `ANALYZE`), the database will make terrible decisions because it thinks the table is empty or full when it isn't.\n",
    "\n",
    "#### Step 1: Hypothesis\n",
    "We will deceive Postgres. We will insert data, check the statistics, then delete data, and see if Postgres notices.\n",
    "\n",
    "#### Step 2: The Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02782e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_postgres_table() # Starts with autovacuum OFF\n",
    "\n",
    "stats_log = []\n",
    "\n",
    "with psycopg2.connect(**DB_PARAMS) as conn:\n",
    "    conn.autocommit = True\n",
    "    with conn.cursor() as cur:\n",
    "        \n",
    "        # 1. Insert Data\n",
    "        print(\"Inserting 100k rows...\")\n",
    "        cur.execute(\"INSERT INTO sensor_logs (payload) SELECT 'data' FROM generate_series(1, 100000);\")\n",
    "        \n",
    "        # Manually ANALYZE so it knows about the inserts\n",
    "        cur.execute(\"ANALYZE sensor_logs;\")\n",
    "        \n",
    "        cur.execute(\"SELECT reltuples FROM pg_class WHERE relname = 'sensor_logs';\")\n",
    "        est_rows = cur.fetchone()[0]\n",
    "        stats_log.append({'Phase': '1. After Insert', 'Real Count': 100000, 'DB Estimate': est_rows})\n",
    "        \n",
    "        # 2. Delete Everything (But don't tell the stats collector!)\n",
    "        print(\"Deleting 100k rows (Secretly)...\")\n",
    "        cur.execute(\"DELETE FROM sensor_logs;\")\n",
    "        \n",
    "        # Check Stats WITHOUT Analyzing\n",
    "        cur.execute(\"SELECT reltuples FROM pg_class WHERE relname = 'sensor_logs';\")\n",
    "        est_rows = cur.fetchone()[0]\n",
    "        stats_log.append({'Phase': '2. After Delete (Stale)', 'Real Count': 0, 'DB Estimate': est_rows})\n",
    "        \n",
    "        # 3. Analyze\n",
    "        print(\"Running ANALYZE...\")\n",
    "        cur.execute(\"ANALYZE sensor_logs;\")\n",
    "        \n",
    "        cur.execute(\"SELECT reltuples FROM pg_class WHERE relname = 'sensor_logs';\")\n",
    "        est_rows = cur.fetchone()[0]\n",
    "        stats_log.append({'Phase': '3. After Analyze', 'Real Count': 0, 'DB Estimate': est_rows})\n",
    "\n",
    "print(\"\\nStats Log:\")\n",
    "print(pd.DataFrame(stats_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c4cee",
   "metadata": {},
   "source": [
    "#### Step 3: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(stats_log)\n",
    "\n",
    "x = range(len(df_stats))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar([i - width/2 for i in x], df_stats['Real Count'], width, label='Real Row Count', color='green')\n",
    "plt.bar([i + width/2 for i in x], df_stats['DB Estimate'], width, label='DB Estimate (pg_stats)', color='gray')\n",
    "\n",
    "plt.xticks(x, df_stats['Phase'])\n",
    "plt.title('Statistics Drift: When the DB Hallucinates')\n",
    "plt.ylabel('Row Count')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4351e3",
   "metadata": {},
   "source": [
    "#### Step 4: The Physics\n",
    "In Phase 2, the table was actually empty (Real Count = 0). However, the DB Estimate was still 100,000. If you ran a query joining this table to another, the Optimizer would allocate memory for a Hash Join expecting 100k rows, wasting massive resources on an empty table. `ANALYZE` scans the table to update these statistics. In production, `autovacuum` usually handles this, but heavy load can cause it to lag.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ecc00",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "1. **Deletes are fake**: They are just updates to metadata. Space is not freed until a Vacuum occurs.\n",
    "2. **Small Files kill performance**: Always buffer your writes or run a compaction job to merge small files.\n",
    "3. **Stats must be fresh**: A database with old statistics is like a driver using a map from 1990."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
